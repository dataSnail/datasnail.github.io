layout: post
categories: Papers
title: DeepCas：an End-to-end predictor of information Cascades 笔记
author: datasnail
show: index
comments: true
tags:

- 论文阅读

## NoTE 4 【DeepCas：an End-to-end predictor of information Cascades】

[TOC]

## 一、文章摘要

虽然现有研究指出，信息级联的一些重要属性是可以被预测的，例如size，growth，shape等。但是这些都是基于手工提取特征，手工特征必须仔细设计，不利于模型的推广。

所以，基于最近深度神经网络的发展和成功，作者探索了是否能设计一种end-to-end的模型，来预测传播的规模（或者叫流行度，原文：future size of cascade）。

> We find that node embeddings fall short of predictive power, and it is critical to learn the representation of a cascade graph as a whole.
>
> 作者发现节点的嵌入表示在此预测过程中不太起作用，而把cascade graph作为一个整体来表示是十分重要的。

## 二、问题描述

给定$t_0$时刻的社交网络快照$\mathcal{G}=(V, E)$，$V$是对应的节点（用户or作者），边$E$代表节点之间在$t_0$时刻之前的关系（一般指动态关系，例如转发、引用）。设定$C$是在$\mathcal{G}$中，$t_0$时刻后的级联传播集合，$c \in C$是其中一条传播快照，持续时间为$t$，其传播图$g_{c}^{t}=\left(V_{c}^{t}, E_{c}^{t}\right)$，其中$V_{c}^{t}$是$V$的子集，是在时间$t$内，级联$c$增加的用户；$E_{c}^{t}=E \cap\left(V_{c}^{t} \times V_{c}^{t}\right)$是两端节点都在$V_{c}^{t}$中的边的集合。

目标是预测在$\Delta t$时间内，级联$c$增加的人数$\Delta s_{c}=\left|V_{c}^{t+\Delta t}\right|-\left|V_{c}^{t}\right|$。也就是要设计函数$f$，把$g_{c}^{t}$映射到$\Delta s_{c}$，$f: g_{c}^{t} \rightarrow \Delta s_{c}$。

## 三、模型思路

整体框架如下：

![deepcas_framework](E:\SEU2\document_mq\NoTE Series\figures\deepcas_framework.png)

<center>图1 整体框架</center>
**（1）首先**，图1中从（a）到（b）的过程，从给定的传播图$g_{c}$开始，基于此图进行**随机游走**，产生一系列的传播序列。

此部分重点在于随机游走，跟deepwalk的随机游走类似，但是此处给定了每一步随机游走的概率，随机游走依据以下markov chain：

![deepcas_randomwalk](E:\SEU2\document_mq\NoTE Series\figures\deepcas_markov_randomwalk.png)

<center>图2 随机游走的马尔科夫链</center>
**随机游走过程**：S是开始，状态N表示从当前节点转移到邻居节点；此时，下一步会有$1-p_j$的概率仍然停留在此节点，会有$p_j$的概率跳转（J）到下一个节点；当在状态J时，还会有$1-p_0$的概率终止此次随机游走，到达状态T。此时，得到一条随机游走序列。

此处概率$p_j$是节点转移概率，$p_0$代表是否重新开始新的随机游走，计算方法有很多，原文中也给出一种计算方法。**但是，但是**，文中认为这样直接设定不太好，原因是不同的传播图可能需要不同的概率进行随机游走，所以他们先指定了一个随机游走长度$T$和随机游走序列数量$K$，然后展开架势，开始学习。

**（2）其次**，上图（c），使用双向GRU学习到每个序列的表示。其实文中是先得到节点的表示，然后把每个节点的表示组成一个向量，就成了sequence的表示了。具体过程如下：

先由一个从左到右的GRU组成的RNN，学习到节点的表示$h_i$。
$$
\begin{array} u_{i} &=\sigma\left(W^{(u)} x_{i}+U^{(u)} h_{i-1}+b^{(u)}\right) \\ r_{i} &=\sigma\left(W^{(r)} x_{i}+U^{(r)} h_{i-1}+b^{(r)}\right) \\ \hat{h}_{i} &=\tanh \left(W^{(h)} x_{i}+r_{i} \cdot U^{(h)} h_{i-1}+b^{(h)}\right) \\ h_{i} &=u_{i} \cdot \hat{h}_{i-1}+\left(1-u_{i}\right) \cdot h_{i-1} \end{array}
$$

同理，再加一个从右到左的GRU序列，最终把两部分向量拼接起来，得到节点最后的表示$\overleftrightarrow{h}_{i}^{k}$。
$$
\begin{array}{1}{\vec{h}_{i}^{k}=\operatorname{GRU}_{f w d}\left(x_{i}, \vec{h}_{i-1}^{k}\right)} \\ {\overleftarrow{h}_{i}^{k}=\operatorname{GRU}_{b w d}\left(x_{i}, \overleftarrow{h}_{i+1}^{k}\right)} \\ {\overleftrightarrow{h}_{i}^{k}={\overrightarrow{h}}_{i}^{k} \oplus \overleftarrow{h}_{i}^{k}}\end{array}
$$
因为 以上节点表示都是基于一个传播树图来做的，所以，要表示传播树图中的一条序列，就可以把所有的节点按照顺序组成一个向量：$\left[\stackrel{\leftrightarrow}{h}_{1}^{k}, \ldots, \stackrel{\leftrightarrow}{h}_{i}^{k}, \ldots, \stackrel{\leftrightarrow}{h}_{T}^{k}\right]$，此处一个sequence还是$T$个节点。这样我们就得到一个张量，如图1(d)的矩阵表示。

**（3）再次**，图1（d），通过Attention机制，学习参数$K$和$T$。

前面介绍过，随机游走产生序列的时候，要以$1-p_0$的概率停止。从参数学习的角度，我们可以通过检查取样sequence的长度是不是能很好的表示传播树图，来学习参数$p_0$。那直觉上，我们可以把采样的长度为$T$的$K$个sequence，划分到“mini-batch”中，这样可以通过读取更多的“mini-batch”来更好的学习传播树图的表示。所以，为了实现这个直觉的方法，文章假设在“mini-batch”之间的attention是几何分布的，那么如果sequence在传播树图$g$的第一个mini-batch中，所带的权重为$p_{geo}^c$，在下一个mini-batch中具有的attention就是$(1-p_{geo}^c)p_{geo}^c$，并且以此类推，如下图所示：

![deepcas_attention](E:\SEU2\document_mq\NoTE Series\figures\deepcas_attention.png)

<center>图3  用attention来集成图表示</center>
理论上，如果我们采样sequence的数量$K \rightarrow \infin$，每个mini-batch所得到的attention权重为$1 / p_{geo}^c$。所以通过这个期望，可以学习到需要采样多少个sequence。文章中指出，如果我们去为每个传播树图，拟合一个自由变量（free parameter）的话，可能自由度就太高了，所以基于观察，为每个传播树图指定了跟规模相关的参数。即，如果用$sz(g_c)$表示图的规模，跟规模相关的参数$\left\lfloor\log _{2}\left(\mathrm{sz}\left(g_{c}\right)+1\right)\right\rfloor$，那么$p_{geo}^c$可以由$p_{g e o}^{\left\lfloor\log _{2}\left(\operatorname{sz}\left(g_{c}\right)+1\right)\right\rfloor}$代替。

**（4）最终**，基于以上步骤和结果，得到一个图的表示形式，如果mini-batch size设置为$B$，那么第$k$个sequence会进入$(\lfloor k / B\rfloor+ 1)$-th mini-batch进行训练。所以，加上attention机制，图$g_c$的长度为2H的向量表示为：
$$
h\left(g_{c}\right)=\sum_{k=1}^{K} \sum_{i=1}^{T}\left(\left(1-a_{c}\right)^{\lfloor k / B\rfloor} a_{c}\right) \lambda_{i} \overleftrightarrow{h}_{i}^k
$$

其中，$a_{c}=p_{g e o}^{\left\lfloor\log _{2}\left(\operatorname{sz}\left(g_{c}\right)+1\right)\right\rfloor}$，此处$a_c,\lambda_i$都是通过学习得到。

（5）最后的最后，用一个MLP来得到结果。即，$f\left(g_{c}\right)=\operatorname{MLP}\left(h\left(g_{c}\right)\right)$，用MSE来衡量损失，损失函数为：
$$
\mathrm{MSE}=\frac{1}{n} \sum_{i=1}^{n}\left(\hat{y}_{i}-y_{i}\right)^{2}
$$

==这里得到的$y_i$是真实增量$\Delta{s}_i$的一个缩放版本，即$y_{i}=\log _{2}\left(\Delta s_{i}+1\right)$。==[原文中说3.1中有原因，暂时没找到为什么要这么做。]

## 四、实验部分

### 4.1 数据集：

Twitter和Aminer的数据集 https://www.aminer.cn/citation 。

Twitter数据集：从（2016年）6.1到6.15的推文用来训练，并且在10天内至少转发过1次的。并且只有一次转发的，通过下采样至5%。6.16时间段数据集用来验证，6.17~6.20时间段的数据用来测试。

**处理数据集的trick**：文中由于没有获得用户间的follower/followee关系，采用了文献[2]的方法，即如果A转发了B的推文或者是在推文中提及（@）了B，那么就构建一条从A到B的边。相比于follower/followee关系网，这种重建的网络还有优势呢：**重建的网络累积了所有的信息流，而且反映了用户之间真正活跃的联系**。

**预测未来时间段在两个数据集中也有区别**：Twitter是未来1,3,5天；而Aminer是未来1,3,5年的。毕竟转发推文要比写文章简单多了。

**数据集的下采样**：由于在两个数据集中，转发树的增长还是遵循着幂律分布的，所以很多转发树后期直接没有数量上的增长，所以文章从零增长的转发树图中，下采样50%样本。并指出文章[3] [4]也是这么做的。

### 4.2 实验结果

<center>表1 deeplearning和线性模型对比</center>
![deepcas_results](E:\SEU2\document_mq\NoTE Series\figures\deepcas_results.png)

文中首先抛出一个实验结果说：如果我们能找到好的特征集合的话，深度学习也不一定比线性模型强。深度学习模型的优势是我们可以设计一个end-to-end的模型，而不需要手工设计。【这句话说得很trick，没有否定深度学习，也没有否定传统的机器学习。】

<center>表2 结果对比</center>
![deepcas_results_twitter](E:\SEU2\document_mq\NoTE Series\figures\deepcas_results_twitter.png)

Node2Vec和Embedded-IC结果不是很好，作者把他归结为他们是在节点层次进行embedding的，迎合摘要里说的要从整体的层次（传播树图）进行表示才会有好的结果。而另一方面WL-degree和WL-id仅仅从节点度和节点id的角度，进行图核相似度表示，也得不到很好的结果。

接下来就是自己模型和变体进行比较，GRU-bag, GRU-fixed,和 GRU-root，通过比较有三点发现：

1. 采样路径集合来表示一个图，而不是用节点表示的平均值来表示图，这对于结果影响起到至关重要的作用；
2. 进行随机游走的时候，训练随机游走序列的长度和数量，要比不训练结果要好。并且diss了一下deepwalk就是不训练随机游走序列的长度和数量的。sequence的长度和数量可能与图的复杂程度和影响力有关系，因为如果一个级联图很复杂或者是更具有影响力，可能就需要更长而且更多的sequence去表示它。
3. 从root进行随机游走可能不是个足够好的方法（尽管好多方法都这么做）。随机的跳转到其它节点可能会使得graph的表示包含更多传播结构的信息量，而且能更好的处理丢失信息。这个现象跟文献[5]中使用的“神秘”属性有关系。【i.e., whether some early adopters are not directly connected to the root.作者这句话的意思是早期的一些接收者（传播者）可能不直接与root节点连接。】

## 五、可视化解释Embedding

//TODO

## 六、感想

随机游走的方式可以借鉴，首先，通过设计转移概率，更好的学习网络中的转发过程；其次，通过集合节点表示，构造sequence的向量表示，并通过在各层使用attention方法构建最终传播图的表示。这种方法从整体的角度对传播树图进行表示，是从整体层面考虑问题的有效性的一个证实。

## 七、参考文献

[1]   Li, Cheng, et al. "Deepcas: An end-to-end predictor of information cascades." *Proceedings of the 26th international conference on World Wide Web*. International World Wide Web Conferences Steering Committee, 2017. 

[2]  Ratkiewicz, Jacob, et al. "Truthy: mapping the spread of astroturf in microblog streams." *Proceedings of the 20th international conference companion on World wide web*. ACM, 2011. 

[3]  Kupavskii, Andrey, et al. "Prediction of retweet cascade size over time." *Proceedings of the 21st ACM international conference on Information and knowledge management*. ACM, 2012. 

[4]  Tsur, Oren, and Ari Rappoport. "What's in a hashtag?: content based prediction of the spread of ideas in microblogging communities." *Proceedings of the fifth ACM international conference on Web search and data mining*. ACM, 2012. 

[5]  Cheng, Justin, et al. "Can cascades be predicted?." *Proceedings of the 23rd international conference on World wide web*. ACM, 2014. 