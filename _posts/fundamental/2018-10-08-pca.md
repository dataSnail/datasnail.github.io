---
layout: post
categories: Fundamental
title: 主成分分析浅析
author: datasnail
show: index
comments: true
intro: 理解PCA，从此做起。此外我还会开一篇新blog，介绍下Kernel PCA。
tags:
- machine learning
---
PCA方法是将一个N维向量变换成K维向量，目标是选择一组K维的正交基，然后将这N维向量映射到这组基上，得到的K维向量之间协方差为0，而使得方差尽可能的大。
### **一、PCA基本数学知识**
#### **1、方差**  
我们在特征选择的博文里介绍过，方差就是衡量**随机变量**和其**数学期望**之间的**偏离程度**。在特征选择中，可以默认去除方差没有达到阈值的特征，而去选择方差尽可能大的一组特征。这是因为如果方差很小，证明此维特征的差异性很小，则对最后结果的区分性也不是很大。方差的计算公式如下： 
 
$$var(x)=\sigma^2=\frac{\sum(X-\mu)^2}{N}$$  

其中，$$\sigma^2$$为总样本方差，$X$为变量，$\mu$为总样本均值（期望值），$N$为总样本数。  
#### **2、协方差**  
协方差用于衡量两个变量的总体误差。

$$Cov(X,Y)=\frac{\sum_{i=1}^{n}(X_i-\overline{X})(Y_i-\overline{Y})}{n-1}$$

其中$\overline{X},\overline{Y}$是样本$X$和$Y$的平均值。
我们能看出方差是协方差的一种特殊情况，即当两个变量相同时，$Cov(X,X)=var(X)$。但是协方差只能呈现两个变量的**是否**相关，协方差大于0，两个变量正相关；协方差小于0，两个变量负相关；表示不了相关的具体程度，例如两个变量相关性很小，但是都比较分散（方差较大），这样导致算出的协方差就会较大。  

#### **3、线性变换**

#### **4、特征值和特征向量**


### **二、PCA原理介绍**

PCA（Principal Component Analysis）,即主成分分析方法，是使用比较广泛的一种降维方法。  
1. 均值化
2. 求协方差矩阵
3. 求协方差矩阵的特征值即对应的特征向量
4. 将特征向量按照从大到校的顺序排列，取前K个组成矩阵P
5. PX即为降维到K维后的数据

### **三、举个栗子**

