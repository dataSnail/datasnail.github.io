---
layout: post
categories: Fundamental
title: 特征工程
author: datasnail
comments: true
show: backup
intro: 特征工程
paperTime:
tags:
- 基础知识
---

####  **一、特征选择**

发散性：如果特征的方差为接近0，这说明样本在这个特征上差异性很小，则对于样本的区分性也会比较小。
相关性：计算特征与最终目标的相关性，如果相关性高，那么我们应该优先选择这个特征。

特征选择的形式可以分为三种：
Filter：过滤法，通过选择某些统计特征进行过滤，例如，方差、相关系数。当然也可以限制特征个数，或者设定特征选择的阈值来筛选特征。

Wrapper：包装法，根据目标函数选择特征或者排除特征。

Embedded：嵌入法，首先使用机器学习的算法和模型进行训练，得到各个特征的权值系数，然后根据系数的重要性对特征进行选择。例如线性回归的特征系数，决策树中特征的信息增益。

#### 1.过滤法：
在计算时间上比较高效。但是容易选择冗余的特征，因为在过滤特征的时候不注重特征之间的关联关系。Guyon 和Elisseeff讨论了过滤过程中的冗余变量问题(Guyon I 2003)。可能会错过组合型特征，例如某个特征的分类效果很差，但是和其他特征组合起来能得到较好的效果。
相关系数：Pearson相关系数衡量线性相关、Spearman相关系数衡量曲线相关、Kendall相关系数很亮两个变量观测排序的一致性。对于相关和不相关，在阈值设定上是有非常大的主观性的。

局部加权回归模型（LOESS），对于区域样本使用多项式回归（Cleveland W1988）。这类局部回归有极强的适应 性，可以有效得到平滑的回归趋势。


参考：
[1] http://www.cnblogs.com/jasonfreak/p/5448385.html
[2] http://blog.peachdata.org/2017/02/08/selection1.html
[3] 