---
layout: post
categories: Fundamental
title: 贝叶斯及贝叶斯网络
author: datasnail
comments: true
show: index
tags:
- markov
- 基础知识
---

	//2017-10-30 增补两个小例子

我们有一个目标是：**在某个随机变量的集合$\mathcal{X}=\{ X_1,X_2,...,X_n \} $上表示联合分布$P$。**

但是有一个困难，即使每个变量只有两种情况（binary-valued），那么n个变量的联合分布也需要$2^n-1$个数字。所以这种表示十分复杂，数据量大也难于存储。

那如何处理呢？虽然说世间万物互相联系，但是有很多“物”之间的联系并不是很强烈，我们可以认为这两种“物”是独立的。例如,A：今天金价上涨（涨和不涨）；B：今天早上要刷牙（要和不要）；这两者在分析中可以认为是互不影响的，即不会因为今天的金价上涨，我早上就要刷牙；而不上涨，我就不用刷牙；换句话说可以认为两者“风马牛不相及”。此时我们可以说$A \perp B$。

用一个课本中的例子，Diffculty代表课程的难度，Intelligence代表学生的智力，Grade代表学生的成绩高低，Letter代表能拿到推荐信的概率，SAT是能在这门课上拿高分的概率：

{:.center}
![student_instance](/postimg/student_instance.jpg)

可以看到，这个图上显示出来的独立性：SAT拿高分只与此学生的智力(intelligence)有关系，而跟另一门课难易(difficulty)无关，即$Intelligence \rightarrow SAT$ ,而课程的成绩Grade与这门课的难度Difficulty和学生智力Intelligence都有关，而与SAT成绩无关等等。

以上这个图就是一个贝叶斯网。贝叶斯网的各个节点的条件概率表示如下：

{:.center}
![student_instance](/postimg/bayes_network_instance.jpg)


#### **因果推理（causal ressoning）或者预测（prediction）实例：**

##### 1.<课本上例子1>

计算$P(l^1)\approx 0.502$、$P(l^1\|i^0)\approx0.389$、$P(l^1\|i^0,d^0)\approx0.513$,书上只给了结果，自己刚开始算的时候老是会算错，算错的结果是0.3575（是因为没加公式的红色部分，而且还自作聪明的给归一化了）。下面给出$P(l^1\|i^0)$的计算方法，计算正确的可以忽略这部分。

$\begin{split} 
P(l^1\|i^0) &= P(l^1\|g^1)\*P(g^1\|i^0,d^0)\*\color{red}{P(d^0)} + P(l^1\|g^1)\*P(g^1\|i^0,d^1)\*\color{red}{P(d^1)}
\\\\ &+P(l^1\|g^2)\*P(g^2\|i^0,d^0)\*\color{red}{P(d^0)} + P(l^1\|g^2)\*P(g^2\|i^0,d^1)\*\color{red}{P(d^1)}
\\\\ &+P(l^1\|g^3)\*P(g^3\|i^0,d^0)\*\color{red}{P(d^0)} + P(l^1\|g^3)\*P(g^3\|i^0,d^1)\*\color{red}{P(d^1)}
\end{split}$

把上面这个式子整理成另一个形式，即：

$\begin{split} 
P(l^1\|i^0) &= P(l^1\|g^1)\*[ \underbrace{ P(g^1\|i^0,d^0)\*\color{red}{P(d^0)} + P(g^1\|i^0,d^1)\*\color{red}{P(d^1)}}\_{P(g^1\|i^0 )} ]
\\\\ &+ P(l^1\|g^2)\*[ \underbrace{ P(g^2\|i^0,d^0)\*\color{red}{P(d^0)} + P(g^2\|i^0,d^1)\*\color{red}{P(d^1)}}\_{P(g^2\|i^0)} ]
\\\\ &+ P(l^1\|g^3)\*[ \underbrace{ P(g^3\|i^0,d^0)\*\color{red}{P(d^0)} + P(g^3\|i^0,d^1)\*\color{red}{P(d^1)}}\_{P(g^3\|i^0)} ]
\end{split}$

上面每个概率在，上图中均能找到。应用贝叶斯网的链式法则，则可以求得$P(l^1\|i^0)\approx0.389$

三个大括号表示全概率公式，全概率公式是这样的：$P(A)=P(A\|B^1)P(B^1) + P(A\|B^2)P(B^2) + ... + P(A\|B^n)P(B^n)$。

##### 2.<课本上例子2>
假设一个公司，试图招聘George，招聘官相信George有30%的可能是高智商（$P(i^1)=0.3$），现在了解到他的特定的课程成绩是C(g^3)，此时George的高智商概率突然就已经是7.9%了，即$P(i^1\|g^3)\approx0.079$。

怎么得出的呢？计算正确可略过一下内容：
我们已知的，还是在上图中体现出来，尤其是$P(i^1)=0.3$；现在我们要求的是$P(i^1\|g^3)$，由于我们看到的没有i在g条件下的概率，所以我们先使用一个贝叶斯公式：
$
P(i^1\|g^3)=\frac {P(g^3\|i^1)\*P(i^1)} {P(g^3)}
$

应用上一步很简单，此时我们直接就有$P(i^1)$，在解决$P(g^3\|i^1)$和$P(g^3)$，很显然也是用到全概率公式：

$ P(g^3\|\color{blue}{i^1}) = P(g^3\|\color{blue}{i^1},\color{red}{d^0})\*\color{red}{P(d^0)} + P(g^3\|\color{blue}{i^1},\color{red}{d^1})\*\color{red}{P(d^1)} = 0.092$

$ P(g^3\|\color{blue}{i^0}) = P(g^3\|\color{blue}{i^0},\color{red}{d^0})\*\color{red}{P(d^0)} + P(g^3\|\color{blue}{i^0},\color{red}{d^1})\*\color{red}{P(d^1)} = 0.46$

$ P(g^3) = P(g^3\|\color{blue}{i^1})\*P(\color{blue}{i^1}) + P(g^3\|\color{blue}{i^0})\*P(\color{blue}{i^0}) = 0.0789$

利用以上三个公式（公式右边的都在图里），就可以算出最终结果。通过以上两个例子，应该在这个图里所有的全概率和条件概率都会求了。

	//2017-03-28

**注：以下均来源于周老师的机器学习西瓜书，详细内容见第七章：贝叶斯分类器**

#### 1、朴素贝叶斯分类器
**朴素贝叶斯分类器**：贝叶斯公式$$P(c \mid x) = \frac {P(c) P(x \mid c) } {P(x) } $$来估计后验概率 $$P(c \mid x)$$的主要困难在于：类条件概率$$P(x \mid c)$$的所有属性上的联合概率，难以从有限的训练样本中估计而得。为避开这个障碍，朴素贝叶斯分类器采用了“属性条件独立性假设”（attribute conditional independence assumption）：对已知类别，假设所有的属性相互独立。换言之，假设每个属性独立的对分类结果发生影响。<sup>[1]</sup>

基于此假设，贝叶斯公式可重写为：

$$P(c \mid x) = \frac {P(c) P(x \mid c) } {P(x) } = \frac {P(c)} {P(x) } \prod_{i=1}^d P(x_i \mid c)$$

可利用**拉普拉斯修正**，避免因训练集样本不充分而导致概率估值为零的问题。

#### 2、半朴素贝叶斯分类器

**半朴素贝叶斯分类器** :朴素贝叶斯采用的属性条件独立性假设，在现实中很往往很难成立。于是，人们尝试对属性条件独立性假设进行一定程度的放松，产生了“半朴素贝叶斯分类器”。适当考虑一部分属性间的相互依赖信息，从而既不需要进行完全联合概率计算，又不至于彻底忽略了比较强的属性依赖关系。“独依赖估计”（One-Dependent Estimator,简称ODE)是半朴素贝叶斯分类器最常用的一种策略。“独依赖”是假设每个属性在类别之外最多仅依赖于一个其他属性,即：

$$P(c \mid x) 正比  P(c) \prod_{i=1}^d P(x_i \mid c, p a_i)$$

NB：

SPODE：设置超父节点。

TAN：通过在最大带权生成树的基础上，通过条件互信息建立依赖关系。

AODE（Averaged One-Dependent Estimator）[webb et al.,2015]:一种基于集成学习机制，更为强大的独立依赖分类器。与SPODE通过模型选择确定超父属性不同，AODE尝试将每个属性作为超父来构建SPODE，然后将那些具有足够训练数据支撑的SPODE集成起来作为最终结果。

#### 3、贝叶斯网

**贝叶斯网**：也称为“信念网”（belief network），借助于无环图DAG，来刻画属性之间的依赖关系。用条件概率表（CPT）来描述属性之间的联合概率分布。

**结构**：tail-to-tail 、 head-to-head 、 head-to-tail

**学习**：情况一、如果网络结构已知，则贝叶斯网络的学习过程相对简单，只需要通过对训练样本“计数”，估计出每个节点的条件概率表即可。
情况二、不知道网络结构，则贝叶斯网络的学习首要任务就是根据训练的数据集找出结构最“恰当”的贝叶斯网。“评分搜索”是求解这一问题的常用方法。

**推断**：最理想的是直接根据贝叶斯网定义的联合概率分布来精确地计算后验概率。不幸的是，“精确推断”被证明是NP-难。当网络节点较多，连接稠密时，难以进行精确推断，此时借助“近似推断”通过降低精度要求，在有限的实践内求得近似解。通常使用吉布斯采样来完成。

吉布斯采样如何工作：


[1]. 周志华. 机器学习[M]. Qing hua da xue chu ban she, 2016.
[2].
