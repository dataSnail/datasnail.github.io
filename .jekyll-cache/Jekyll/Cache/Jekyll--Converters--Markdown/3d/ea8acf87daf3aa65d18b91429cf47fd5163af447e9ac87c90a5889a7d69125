I"-<ol>
  <li>解决了什么痛点？</li>
  <li>用了什么特征，什么方法？</li>
  <li>方法所牵扯的数学知识？</li>
  <li>方法的理论性？</li>
</ol>

<p>在实验中经常遇到把连续变量离散化的需要，而且在很多模型中也需要把某个连续的特征值转变为离散的值，这样也能增强模型的鲁棒性。而不曾想到现在用的方法竟然是1993年的一篇文章.</p>

<p class="center center"><img src="/postimg/mdlp/paper_head.png" alt="" /></p>

<h3 id="一简介"><strong>一、简介</strong></h3>
<p>将连续的数据进行最优离散化是一个NP-hard问题。这篇文献提出了一种<strong>基于熵最小化的启发式算法</strong>。</p>

<p>证明适用性。
通过多个数据集的实证，通过这种方法能获得更好的决策树。</p>

<p><span style="color:red">分类学习算法通常使用启发式方法来指导他们的搜索，在属性值和类之间关系的巨大空间内</span>。其中一个这样的启发式方法就是，局部的特征选择，通过最小化在数据集中类的信息熵。（例如ID3、CART、C4）</p>

<p>属性在学习问题中可能是名词，也可能是连续的值。之前提到的属性选择都是假设所有的属性都是名词性的。而连续值的属性需要在选择之前离散化，就是把属性划分到不同的子域中。<br />
这篇文章就是<strong>只关注与连续属性的离散化</strong>。</p>
<ol>
  <li>首先，提出一种<strong>二分离散化（binary discretization）的信息熵最小化的启发式方法</strong>。[更好的理解启发式、提供证明、效率更高]</li>
  <li>其次，拓展上述方法到多个区间的离散化，通过实证试验证实能构造出更好的决策树。</li>
</ol>

<h3 id="二binary-discretization"><strong>二、Binary Discretization</strong></h3>

<p>连续的值通常在决策树的生成过程中被离散化成两部分。一个阈值T，把连续的属性A分成两部分，A&gt;T的部分和A&lt;T的部分。T称为cut point（割点）。这在ID3、GID3、CART等树种都是这么做的。虽然我们展示的结果应用在了一般离散化方法中，但是他们都是呈现在特定的自上而下的决策树生成过程中。【不具有普适性】。</p>

<p>假设我们有一个节点，把N个样例分成S个和N-S个样例。对于每个连续的属性，我们要从所有的候选割点中选择一个“最好”的割点$T_A$，所以呢，首先要把这个连续特征<strong>排序</strong>，每个中间的点都可能是候选的割点。所以，如果属性的N个值都不相同，就要评估N-1次。每次评估，都要按照割点T，计算下左边的熵值和右边的熵值，然后一一比较，最终做出选择。对于树的每个节点我们都要这么做。</p>

<p>假设T分割了S个集合成两个子集$S_1,S_2$。假设有k个类$C_1,…,C_k$，$P(C_i,S)$是样例S中有$C_i$类的比例。那么集合S的类熵值可以定义为：</p>

<script type="math/tex; mode=display">Ent(S)=-\sum^{k}_{i=1}P(C_i,S)log(P(C_i,S))</script>

<p>这里对数的底为2，Ent(S)量化了区分S中特定的类，所需要的信息量，单位为bits。为了衡量在S分为$S_1,S_2$后结果的类熵值，我们取它们类熵值结果的加权平均值。这里给出分区信息熵的定义，然后就知道对于二分离散化问题，如何寻求最合适的割点了。<br />
<strong>定义一：</strong>由割点T引起的分区信息熵： $E(A,T;S)=\frac{|S_1|}{|S|}Ent(S_1)+\frac{|S_2|}{|S|}Ent(S_2)$<br />
那么对属性A的二分离散化，<span style="color:red"><strong>就是选择一个割点$T_A$使得$E(A,T_A;S)$在所有的候选割点中最小。</strong></span></p>

<h4 id="21-割点的讨论"><strong>2.1 割点的讨论</strong></h4>
<p>//TODO</p>
<h4 id="22-割点总是在边界上"><strong>2.2 割点总是在边界上</strong></h4>
<p>//TODO
这里就是证明给出的割点，总是在属性A的两个不同类别的值之间的。<br />
这样也可以减少候选割点的数量，进一步减少了运算量。</p>
<h3 id="三方法的泛化"><strong>三、方法的泛化</strong></h3>
<p>泛化的动机就是能得到更好的“树”。<strong>训练数据排序后，算法迭代执行，每次总是选择最好的割点。</strong>既然是迭代执行，肯定要有一个不再继续进行二分类离散化的停止标准。这个标准要是合乎原则，且在理论上是合理的。</p>

<p><strong>为什么从一个节点划分多个区间要比划分两个区间，生成的树更为精确呢？</strong></p>

<p>通常，有意思的区间通常在属性数据范围的内部或者中间。因此，想要得到这样的区间，每次进行二分类，就会导致在这个有趣区间之外的过渡划分。举个栗子：比如对于属性A，属性值的区间为[0,40]，而[12,20]这个区间是有趣的。假设A的范围数据被离散化为{(-inf,12),[12,20),[20,25),{25,inf)}。给定一个算法，例如GID3，它能够过滤掉不相关的值，原则上很可能得到下图(a)中的一棵树。属性选择算法确定了四个区间中的两个是相关的。在这个区间外的样例被分到图中标为s的子集中。</p>

<p class="center center"><img src="https://i.imgur.com/1lhwntI.png" alt="" /></p>

<p>只用二分离散化的算法，为了选出这两个区间，就会生成图(b)中所示的决策树。可以看到集合S现在不必要的备份出S1和S2两个子集。</p>

<p>第一棵树，算法在集合S中，可能会选择使用其他更为合适的属性来划分。而这种选择，不可能出现在第二种情况中了【因为集合S已经被划分成S1和S2了】，将来属性的选择要基于更小的子集（S1,S2）。本质上，这将导致同样的排序问题，就像那些在文献[2,5]中由不相关的值问题导致的一样。GID3如何处理这个问题的和只有一个子集如何进行分支的，详情请看文献5。</p>

<h4 id="31--to-cut-or-not-to-cut-that-is-the-question"><strong>3.1  To Cut or not to Cut? That is the Question</strong></h4>
<p>接下来就是要解决要不要接受某个割点T的问题了。这个问题很自然的可以被形式化成二类的决策问题：接受还是不接受切割方案$\pi_T$。HT代表假设$\pi_T$被接受这个命题。那么也就是说，HT是一个分类器，测试A的值跟阈值T的关系，分类出在样本E中，A的属性值小于T的样本。类似，NT代表一个空假设，如果$\pi_T$被拒绝，就会得到一个结果。因此NT将会分类出在E中的所有样本，而不用检查A的值。由于只有“接受”或者“拒绝”两个行为，不论什么情况，其中的一个肯定是正确的，而另一个肯定是错误的。当然，我们没有方法直接判断哪个是正确的。。。。</p>

<p>$d_A$是接受划分$\pi_T$，$d_R$是拒绝它。决策集合D={$d_A$，$d_R$}。如果设定做出错误决策的成本函数，那么与决策规则相联系的期望成本就可以表示为如下式子：</p>

<p class="center center"><img src="/postimg/discretization/formular.png" alt="" width="450" /></p>

<p>$c_{11}$，$c_{22}$代表做出正确的决策的成本，$c_{12}$，$c_{21}$是做出错误决策的成本。这是期望贝叶斯风险，不管是用何种决策来选择。贝叶斯决策标准，要求选择最小化预期成本的决策规则。</p>

<p>由于不知道该给$c_{12}$，$c_{21}$赋什么样的值，我们采用统一错误成本分配。如果$c_{11}=0$，$c_{22}=0$，$c_{12}=1$，$c_{21}=1$，那么最小化贝叶斯风险就蜕变为所熟知的误差概率准则（Probability-of-ErrorCriterion,PEC），就是最小化做出做出错误决策的概率。在文献[12]中，可以看到简单的推导，贝叶斯决策准则退变采用这种决策规则：给定数据集S，对P（HT|S）在所有假设中是最大的，选择假设HT。我们指这种决策标准是贝叶斯决策，这种策略通常也叫作最大后验标准（MAP），也相当于PEC。</p>

<p>在我们的决策问题中，贝叶斯决策要选择决策d，依赖决策在给定数据集S条件下有最大的概率：因为我们应该选择d_A，当且仅当Prob{HT|S}&gt;Prob{HT|S}。如果有一种方法能确定以上的两个概率，那我们的问题也就解决了：简单的选择在给定数据的条件下具有较大概率的假设，这是最好的策略。不幸的是，没有简单的方法来直接计算这些概率。然而我们应该选择一种方法来让我们间接的估计那个概率大一些。</p>

<h4 id="32-the-minimum-description-length-principle"><strong>3.2 The Minimum Description Length Principle</strong></h4>

<p>最小化描述长度的目标定义为，对所有的目标而言，来唯一标识一个目标，需要的最小bits数量。<br />
在我们的决策问题中，我们能使用最小的描述长度准则，给定固定样例集合，在有较高概率的假设上做一个猜想。MDLP是一种普遍的原则，它旨在将科学中的自然偏差编码为更简单的解释相同数据体的理论。MDLP最早是Rissanen在文献[17]中提出来的，后来又被文献[14,18]应用。在这使用文献[14]的定义：</p>

<p>定义3：给定一系列的假设和一个集合数据S，最小描述长度准则就是选择假设HT，它的$MLength(HT)+MLength(S|HT)$是最小的在所有假设里。MLength(HT)表示HT的最小可能编码的长度，MLength(S|HT)是给定假设，数据的最小编码长度。</p>

<p>方便起见，假设长度单位为bits。给定假设的条件下编码数据可以被认为是编码对于假设HT而言是例外的数据点。如果HT完全拟合了数据，那么后一项就是0了。<br />
MDLP原则就是不需要调用一些与之前讨论的决策准则不一样的东西。【？？？】可以很容易的看出MDLP和贝叶斯风险最小策略（在统一错误损失的假设下？？？）在理论上互相关联的。由于篇幅有限，我们省略了推导，而简化表达式展开，在给定数据S条件下，特定假设H所需的bits位数为：用贝叶斯理论得到，$-log_{2}(Prob{H|S})$。最终得到的表达式就与MDLP相同。这将成为采用MDLP的动机，因为它减少了我们采用其他启发式的方法任意性来决定，何时避免进一步的划分。</p>

<p>基于我们先前的参数，如果在给定数据下，我们有一个方法能找到一个真的最小的编码长度的假设，那么利用MDLP，来选择各假设集合中的一个，使得选择的假设有最大的后验概率。因此，这就等同于PEC决策准则。这意味着，选择的将是一个能最小化错误决策概率的假设。 然而，在物理世界，我们没法获得概率分布。所以MDLP就用来估计损失或者用来启发，来区分不同的假设。</p>

<h4 id="33-应用mdlp一个编码问题"><strong>3.3 应用MDLP:一个编码问题</strong></h4>
<p>讨论了在NT和HT假设下的编码度量。</p>

<h4 id="34-决策标准"><strong>3.4 决策标准</strong></h4>
<p>得出来一个式子，解决了什么时候应该接受切分，什么时候应该拒绝。</p>

<h3 id="四实验">四、实验</h3>
<p>用id3树衡量那种切分效果好。</p>

<p>后面几个部分，后来有时间再重新补上吧。</p>

<p>还应该懂的：
香农编码 <br />
最小错误概率判别准则</p>

<h3 id="五参考文献">五、参考文献</h3>
<p>[1]. Fayyad, Usama, and Keki Irani. “Multi-interval discretization of continuous-valued attributes for classification learning.” (1993).</p>
:ET