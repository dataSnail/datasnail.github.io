I"<p>希望之后的我和找到此篇博客的你，看完这个系列都能清晰地弄懂what is隐马尔科夫模型。</p>
<h3 id="一基本概念"><strong>一、基本概念</strong></h3>
<blockquote>
  <p>从概率图模型开始，介绍马尔科夫性质、隐马尔科夫模型。主要参考书：周志华西瓜书、李航统计学习方法、模式分类和一些博客。其中不乏从这些参考书中整理出来的一些句子甚至段落，因为写的太好了。</p>
</blockquote>

<p><strong>机器学习</strong>就是从已观察到的证据来对感兴趣的位置变量进行估计和推测。而<strong>概率模型</strong>（probabilic model）提供了一种描述框架，讲学习任务归结为计算变量的概率分布。在概率图模型中，由已知变量推测位置变量的分布称为“<strong>推断</strong>”（inference），就是如何基于可观测变量推测出来位置变量的条件分布。这里可分为<strong>生成模型</strong>和<strong>判别模型</strong>，具体就不再赘述了。<br />
<strong>概率图模型</strong>是一类用图来表达变量相关关系的概率模型。在概率图中，我们用一个节点来表示一个或一组随机变量，而节点之间的边表示变量之间的概率相关关系。所以整个图就是一个“概率关系图”。<br />
<strong>概率图可以大致分为两类</strong>：<strong>第一类</strong>是有向无环图（DAG）表示变量之间的依赖关系，称为有向图模型或贝叶斯网络（Bayesian Network）；<strong>第二类</strong>是无向图来表示变量间的相关关系，称为无向图模型或马尔可夫网（Markov Network）。其中第一部分我写过<a href="/fundamental/2017/03/28/bayesian.html">贝叶斯及贝叶斯网络</a>的介绍，此处我们介绍另一种比较简单常用的模型—隐马尔科夫模型（Hidden Markov Model，HMM）这是结构最简单的动态贝叶斯网（dynamic Bayesian network），主要用于数据建模，在语音识别、自然语言处理中都有广泛的应用。</p>
<h3 id="二隐马尔可夫模型"><strong>二、隐马尔可夫模型</strong></h3>
<p>隐马尔可夫模型中的变量可以分为两组：一组是状态变量${y_1,y_2,…,y_n}$，其中$y_i \in \mathcal{Y}$表示第i时刻的系统状态。这里通常假定状态变量是隐藏的、不可以被观测到的，因此状态变量也称为隐变量（hidden variable）。第二组是观测变量${x_1,x_2,…,x_n}$，其中$x_i \in \mathcal{X}$表示第i时刻的观测值。如下图：</p>

<p>状态转移概率：</p>

<p>输出观测概率：</p>

<p>初始状态概率：</p>

<p>评估问题、解码问题和训练（学习）问题。</p>
<h4 id="三个问题"><strong>三个问题</strong></h4>
<p>此处只需要了解各个问题的定义和用何种算法解决即可,我们会在接下来的博客中介绍：<br />
<strong>评估问题</strong>：给定模型$\lambda = [A,B,\pi ]$，如何有效计算其产生的观测序列$x={x_1,x_2,…,x_n}$的概率$P(x|\lambda)$？也就是如何评估模型与观测序列之间的匹配程度。此时我们可以使用<strong>前向算法、后向算法</strong>来解决；<br />
<strong>解码（预测）问题</strong>：给定模型$\lambda = [A,B,\pi ]$和观测序列$x = { x_1,x_2,…,x_n  } $，如何找到与此观测序列最为匹配的状态序列$y = y_1,y_2,…,y_n $？也就是如何根据观测序列推断出隐藏的模型状态。此时我们可以用<strong>维特比（Viterbi）算法或者近似算法</strong>解决；<br />
<strong>训练（学习）问题</strong>：给定观测序列$x = { x_1,x_2,…,x_n  } $，如何调整模型参数$\lambda = [A,B,\pi ]$使得该序列出现的概率最大？也就是说如何训练模型时期能更好的描述观测数据，此时我们可以使用<strong>Baum-Welch算法（前向后向算法）</strong>。</p>

<hr />
<center>-----------------------------------------分割线(以下部分未整理）-----------------------------------------</center>
<hr />

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>2017-01-21
</code></pre></div></div>

<p>马尔科夫性质：he future is independent of the past given the present.</p>

<p>马尔科夫链（Markov Chain）又称为马尔科夫过程，是一种离散的随机过程。</p>

<p>马氏链收敛定理</p>

<p>细致平衡方程</p>

<p>贝叶斯网络的到的图是指依次用无向边连接每两个具有共同子节点的节点，并去除其他边的方向而得到的无向图，是一种马尔科夫网络。</p>

<p>蒙特卡洛方法，解决问题过程中，利用大量的随机样本，对这个样本的结果进行概率分析，从而得到问题求解方法，这样的解决方法都可以称之为是蒙特卡洛方法。</p>

<p>对于比较复杂的或者高纬度的分布，利用蒙特卡洛方法生成随机样本是比较困难的，所以要发展一些更为复杂的随机模拟技术。</p>

<p><strong>MCMC（Markov Chain Monte Carlo）</strong>：在蒙特卡洛模拟中，我们在后验分布中抽取样本，当这些样本独立时，利用大数定律样本均值会收敛到期望值。如果得到的样本是不独立的，那么就要借助于马尔科夫链进行抽样。MCMC方法就是为了这个目的而诞生的。</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>2017-03-21
</code></pre></div></div>

<h4 id="1概念马尔科夫网无向图模型马尔科夫场条件随机场"><strong>1.概念(马尔科夫网、无向图模型、马尔科夫场、条件随机场)</strong></h4>

<p><strong>马尔科夫网</strong>：马尔科夫网在计算机视觉领域通常称为<strong>马尔科夫随机场</strong>。被广泛应用于多种视觉处理的任务中，如图像分割、去模糊和去噪、三维重建、物体识别。</p>

<p><strong>马尔科夫随机场</strong>：对于一个无向图模型<script type="math/tex">G</script>，对于其中的任意节点<script type="math/tex">X_i</script>，【以除了他以外的所有点为条件的条件概率】和【以他的邻居节点为条件的条件概率】相等，那么这个无向图就是马尔科夫随机场。</p>

<p><strong>Gibbs分布</strong>：如果无向图模型能够表示成一系列在G的最大团（们）上的非负函数乘积的形式，这个无向图模型的概率分布P(X)就称为Gibbs分布。</p>

<h4 id="2-hammersley-clifford-theorem"><strong>2. Hammersley Clifford Theorem</strong></h4>

<p>Hammersley Clifford Theorem理论认为，<strong>马尔科夫随机场和Gibbs分布是一致</strong>的。</p>

<p>也就是说：
1）Gibbs分布一定满足由node separation导致的条件独立性</p>

<p>2）马尔科夫随机场的概率分布一定可以表示成最大团们上的非负函数乘积形式</p>

<p><strong>条件随机场(Conditional random fields)</strong>：是一种判别式图模型，因为其强大的表达能力和出色的性能，得到了广泛的应用。从最通用角度来看，CRF本质上是给定了观察值集合(observations)的马尔可夫随机场。</p>

<ol>
  <li>有些独立性要求不能用贝叶斯网络表示，因为任意一个这样分布的贝叶斯网I-map都必然包含额外的边，近而导致所期望的这两个独立性条件中的一个无法得到满足。更广义的讲，贝叶斯网要求为每个影响都指定一个方向，如果变量之间的交互影响为对称的，我们期望能够构建一个无需强行为影响指定方向的模型来表示这种相互关系。</li>
  <li>变量节点之间无向边连接，无法再给定其他节点时在一个节点上表示分布的标准CPD。需要更加对称的参数化方法。</li>
</ol>

<p>End</p>

:ET